str(sent.hcluster)
table(grp2 = g25[,"2"], grp5 = g25[,"5"])
#--:
sent.hcluster <- cutree(htree, k = nCluster) #nCluster<-c(2,3)
#---------------------------------------------------------2)
#3)summaritaziton by k means clustering ---------------------------
### k-means (this uses euclidean distance)
sent.dtmmat <- as.matrix(sent.dtm)
### Normalize
#m<-sent.dtmmat
#norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
#m_norm <- norm_eucl(m)
### cluster into int clusters
sent.kmeans <- kmeans(sent.dtmmat, nCluster)
table(sent.kmeans$cluster)
#----------------------------------------------------------------3)
#---------------------------------------------------------------------------------find centers of clusters
str(sent.kmeans$cluster)
str(sent.hcluster)
sent[2]
sent.kmeans$cluster
findCenter <- function(cluster){
clc<-list()
for (i in 1:length(cluster)){
tmp <- try( clc[[cluster[i]]]<-c(clc[[cluster[i]]], i) )
if(class(tmp)=="try-error"){
clc[[cluster[i]]]<-list();
clc[[cluster[i]]]<-c(clc[[cluster[i]]], i)
}
}
return (clc)
}
resc<-findCenter(sent.kmeans$cluster) ################################ sent.kmeans$cluster - sent.hcluster
#resc[[3]] #array of index of doc. in cluster 3
getAvg<-function(ind){ #uses sent.dtmmat
avgdoc<-sent.dtmmat[1,] #term list
centers<-rep.int(0, length(ind) )
for(i in 1:length(ind)){  #for each cluster
for(k in 1:length(avgdoc)){ #initialize average terms list
avgdoc[k]<-0
}
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
for(t in 1:length(avgdoc)){ #for each term
avgdoc[t] <- avgdoc[t] + sent.dtmmat[ as.numeric(ind[[i]][j]) , t]
}
}
#Calculate ranks
rankd<-0
rankd<-rep.int(0, length(ind[[i]]))  #keeps document rank
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
tmpRank<-0
for(t in 1:length(avgdoc)){ #for each term
tmpRank<-(sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*sent.dtmmat[ as.numeric(ind[[i]][j]) , t] ) + (avgdoc[t]*avgdoc[t])
#rankd[j] <- rankd[j] + ( sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*avgdoc[t] )
}
if(tmpRank!=0){
rankd[j]<- sqrt( tmpRank )
}else{
rankd[j]<- 0
}
}
#find highest ranked document's index
#centers[i]<- ind[[i]][ which.max(rankd) ]
centers[i]<- ind[[i]][ which.min(rankd) ]
}
return (centers)
}
centers<-getAvg(resc)
summ<-rep.int(0,length(centers))
for(i in 0:length(centers)){
summ[i] <- sent[as.numeric(centers[i])]
}
summ
#---------------------------------------------------------------------------------find centers of clusters
#trstopword.txt
sname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/trstopword.txt")
con=file(sname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
swtr<- strsplit(line, " ") #TODO
library(stringr)
cname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/data/haber3.txt")
con=file(cname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
line <- line[sapply(line, nchar) > 0] #Remove blank pharagraphs
sentences <- line %>% gsub(" +", " ", .) %>% strsplit(split = "[\\.?!] ") #Get text as sentences grouped according to pharagraps
sent<-sentences[[1]]
c<-0
for (p in 2:length(sentences)) {
if(!is.null(sentences)) {
if (length(sentences[[p]])!=0){
sent<-c(sent,sentences[[p]])
}
}
}
library(tm)
library(Matrix)
sent.list <- c(sent)
sent.corpus<-Corpus( VectorSource( sent.list ) )
#swtrC <- Corpus(VectorSource(as.vector(swtr)))
#Convert all letters to lower case.
sent.corpus <- tm_map(sent.corpus, tolower)
#Remove punctuations.
sent.corpus <- tm_map(sent.corpus, removePunctuation)
#Remove list of stop words in Turkish, from corpus.
sent.corpus <- tm_map(sent.corpus, removeWords, swtr[[1]])
#Erase consequential multiple spaces.
sent.corpus <- tm_map(sent.corpus, stripWhitespace)
#Remove stem words.
sent.corpus <- tm_map(sent.corpus, stemDocument)
#Remove numeric number since they won't be meaningfull for our mining process.
sent.corpus <- tm_map(sent.corpus, removeNumbers)
#Convert to plain text document
sent.corpus <- tm_map(sent.corpus, PlainTextDocument)
#sent.dtm <- DocumentTermMatrix( sent.corpus, control = list(weighting = weightTf, normalize = TRUE))
sent.dtm <- DocumentTermMatrix( sent.corpus, control = list(weighting = weightTf, normalize = TRUE) )
rownames(sent.dtm) = 1:nrow(sent.dtm)
sent.tdm <- TermDocumentMatrix( sent.corpus )
colnames(sent.tdm) = 1:ncol(sent.tdm)
if(length(sent)<1){
nCluster<-0
}else{
nCluster<-sqrt(length(sent))#
}
#2)summaritaziton by hierarhic clustering ------------------------
y <- sparseMatrix( i=sent.tdm$i, j=sent.tdm$j, x=sent.tdm$v, dimnames = dimnames(sent.tdm) )
#colnames(y) <- 1:ncol(y)
htree=hclust(dist(t(y)))
plot( htree )
# Compare the 2 and 4 grouping:
g25 <- cutree(htree, k = c(2,5)) #k=2:5
str(sent.hcluster)
table(grp2 = g25[,"2"], grp5 = g25[,"5"])
#--:
sent.hcluster <- cutree(htree, k = nCluster) #nCluster<-c(2,3)
#---------------------------------------------------------2)
#3)summaritaziton by k means clustering ---------------------------
### k-means (this uses euclidean distance)
sent.dtmmat <- as.matrix(sent.dtm)
### Normalize
#m<-sent.dtmmat
#norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
#m_norm <- norm_eucl(m)
### cluster into int clusters
sent.kmeans <- kmeans(sent.dtmmat, nCluster)
table(sent.kmeans$cluster)
#----------------------------------------------------------------3)
#---------------------------------------------------------------------------------find centers of clusters
str(sent.kmeans$cluster)
str(sent.hcluster)
sent[2]
sent.kmeans$cluster
findCenter <- function(cluster){
clc<-list()
for (i in 1:length(cluster)){
tmp <- try( clc[[cluster[i]]]<-c(clc[[cluster[i]]], i) )
if(class(tmp)=="try-error"){
clc[[cluster[i]]]<-list();
clc[[cluster[i]]]<-c(clc[[cluster[i]]], i)
}
}
return (clc)
}
resc<-findCenter(sent.kmeans$cluster) ################################ sent.kmeans$cluster - sent.hcluster
#resc[[3]] #array of index of doc. in cluster 3
getAvg<-function(ind){ #uses sent.dtmmat
avgdoc<-sent.dtmmat[1,] #term list
centers<-rep.int(0, length(ind) )
for(i in 1:length(ind)){  #for each cluster
for(k in 1:length(avgdoc)){ #initialize average terms list
avgdoc[k]<-0
}
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
for(t in 1:length(avgdoc)){ #for each term
avgdoc[t] <- avgdoc[t] + sent.dtmmat[ as.numeric(ind[[i]][j]) , t]
}
}
#Calculate ranks
rankd<-0
rankd<-rep.int(0, length(ind[[i]]))  #keeps document rank
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
tmpRank<-0
for(t in 1:length(avgdoc)){ #for each term
tmpRank<-(sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*sent.dtmmat[ as.numeric(ind[[i]][j]) , t] ) + (avgdoc[t]*avgdoc[t])
#rankd[j] <- rankd[j] + ( sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*avgdoc[t] )
}
if(tmpRank!=0){
rankd[j]<- sqrt( tmpRank )
}else{
rankd[j]<- 0
}
}
#find highest ranked document's index
#centers[i]<- ind[[i]][ which.max(rankd) ]
centers[i]<- ind[[i]][ which.min(rankd) ]
}
return (centers)
}
centers<-getAvg(resc)
summ<-rep.int(0,length(centers))
for(i in 0:length(centers)){
summ[i] <- sent[as.numeric(centers[i])]
}
summ
#---------------------------------------------------------------------------------find centers of clusters
#trstopword.txt
sname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/trstopword.txt")
con=file(sname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
swtr<- strsplit(line, " ") #TODO
library(stringr)
cname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/data/haber3.txt")
con=file(cname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
line <- line[sapply(line, nchar) > 0] #Remove blank pharagraphs
sentences <- line %>% gsub(" +", " ", .) %>% strsplit(split = "[\\.?!] ") #Get text as sentences grouped according to pharagraps
sent<-sentences[[1]]
c<-0
for (p in 2:length(sentences)) {
if(!is.null(sentences)) {
if (length(sentences[[p]])!=0){
sent<-c(sent,sentences[[p]])
}
}
}
library(tm)
library(Matrix)
sent.list <- c(sent)
sent.corpus<-Corpus( VectorSource( sent.list ) )
#swtrC <- Corpus(VectorSource(as.vector(swtr)))
#Convert all letters to lower case.
sent.corpus <- tm_map(sent.corpus, tolower)
#Remove punctuations.
sent.corpus <- tm_map(sent.corpus, removePunctuation)
#Remove list of stop words in Turkish, from corpus.
sent.corpus <- tm_map(sent.corpus, removeWords, swtr[[1]])
#Erase consequential multiple spaces.
sent.corpus <- tm_map(sent.corpus, stripWhitespace)
#Remove stem words.
sent.corpus <- tm_map(sent.corpus, stemDocument)
#Remove numeric number since they won't be meaningfull for our mining process.
sent.corpus <- tm_map(sent.corpus, removeNumbers)
#Convert to plain text document
sent.corpus <- tm_map(sent.corpus, PlainTextDocument)
#sent.dtm <- DocumentTermMatrix( sent.corpus, control = list(weighting = weightTf, normalize = TRUE))
sent.dtm <- DocumentTermMatrix( sent.corpus, control = list(weighting = weightTf, normalize = TRUE) )
rownames(sent.dtm) = 1:nrow(sent.dtm)
sent.tdm <- TermDocumentMatrix( sent.corpus )
colnames(sent.tdm) = 1:ncol(sent.tdm)
if(length(sent)<1){
nCluster<-0
}else{
nCluster<-sqrt(length(sent))#
}
#2)summaritaziton by hierarhic clustering ------------------------
y <- sparseMatrix( i=sent.tdm$i, j=sent.tdm$j, x=sent.tdm$v, dimnames = dimnames(sent.tdm) )
#colnames(y) <- 1:ncol(y)
htree=hclust(dist(t(y)))
plot( htree )
# Compare the 2 and 4 grouping:
g25 <- cutree(htree, k = c(2,5)) #k=2:5
str(sent.hcluster)
table(grp2 = g25[,"2"], grp5 = g25[,"5"])
sent.hcluster <- cutree(htree, k = nCluster) #nCluster<-c(2,3)
#---------------------------------------------------------2)
#3)summaritaziton by k means clustering ---------------------------
### k-means (this uses euclidean distance)
sent.dtmmat <- as.matrix(sent.dtm)
### Normalize
#m<-sent.dtmmat
#norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
#m_norm <- norm_eucl(m)
### cluster into int clusters
sent.kmeans <- kmeans(sent.dtmmat, nCluster)
table(sent.kmeans$cluster)
#table(sent.kmeans$cluster)
#----------------------------------------------------------------3)
#---------------------------------------------------------------------------------find centers of clusters
str(sent.kmeans$cluster)
str(sent.hcluster)
sent.kmeans$cluster
findCenter <- function(cluster){
clc<-list()
for (i in 1:length(cluster)){
tmp <- try( clc[[cluster[i]]]<-c(clc[[cluster[i]]], i) )
if(class(tmp)=="try-error"){
clc[[cluster[i]]]<-list();
clc[[cluster[i]]]<-c(clc[[cluster[i]]], i)
}
}
return (clc)
}
resc<-findCenter(sent.kmeans$cluster) ################################ sent.kmeans$cluster - sent.hcluster
#resc[[3]] #array of index of doc. in cluster 3
getAvg<-function(ind){ #uses sent.dtmmat
avgdoc<-sent.dtmmat[1,] #term list
centers<-rep.int(0, length(ind) )
for(i in 1:length(ind)){  #for each cluster
for(k in 1:length(avgdoc)){ #initialize average terms list
avgdoc[k]<-0
}
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
for(t in 1:length(avgdoc)){ #for each term
avgdoc[t] <- avgdoc[t] + sent.dtmmat[ as.numeric(ind[[i]][j]) , t]
}
}
#Calculate ranks
rankd<-0
rankd<-rep.int(0, length(ind[[i]]))  #keeps document rank
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
tmpRank<-0
for(t in 1:length(avgdoc)){ #for each term
tmpRank<-(sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*sent.dtmmat[ as.numeric(ind[[i]][j]) , t] ) + (avgdoc[t]*avgdoc[t])
#rankd[j] <- rankd[j] + ( sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*avgdoc[t] )
}
if(tmpRank!=0){
rankd[j]<- sqrt( tmpRank )
}else{
rankd[j]<- 0
}
}
#find highest ranked document's index
#centers[i]<- ind[[i]][ which.max(rankd) ]
centers[i]<- ind[[i]][ which.min(rankd) ]
}
return (centers)
}
centers<-getAvg(resc)
summ<-rep.int(0,length(centers))
for(i in 0:length(centers)){
summ[i] <- sent[as.numeric(centers[i])]
}
summ
#trstopword.txt
sname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/trstopword.txt")
con=file(sname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
swtr<- strsplit(line, " ") #TODO
library(stringr)
cname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/data/haber3.txt")
con=file(cname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
line <- line[sapply(line, nchar) > 0] #Remove blank pharagraphs
sentences <- line %>% gsub(" +", " ", .) %>% strsplit(split = "[\\.?!] ") #Get text as sentences grouped according to pharagraps
sent<-sentences[[1]]
c<-0
for (p in 2:length(sentences)) {
if(!is.null(sentences)) {
if (length(sentences[[p]])!=0){
sent<-c(sent,sentences[[p]])
}
}
}
library(tm)
library(Matrix)
sent.list <- c(sent)
sent.corpus<-Corpus( VectorSource( sent.list ) )
#swtrC <- Corpus(VectorSource(as.vector(swtr)))
#Convert all letters to lower case.
sent.corpus <- tm_map(sent.corpus, tolower)
#Remove punctuations.
sent.corpus <- tm_map(sent.corpus, removePunctuation)
#Remove list of stop words in Turkish, from corpus.
sent.corpus <- tm_map(sent.corpus, removeWords, stopwords('english'))
library(stringr)
cname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/data/haber3.txt")
con=file(cname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
line <- line[sapply(line, nchar) > 0] #Remove blank pharagraphs
sentences <- line %>% gsub(" +", " ", .) %>% strsplit(split = "[\\.?!] ") #Get text as sentences grouped according to pharagraps
#trstopword.txt
library(stringr)
cname <- file.path("D:", "yusuf/UNI/5.1/Text Mining/summarization/data/haber3.txt")
con=file(cname,open="r")
line=readLines(con)
Encoding(line)<-"UTF-8"
line <- line[sapply(line, nchar) > 0] #Remove blank pharagraphs
sentences <- line %>% gsub(" +", " ", .) %>% strsplit(split = "[\\.?!] ") #Get text as sentences grouped according to pharagraps
sent<-sentences[[1]]
c<-0
for (p in 2:length(sentences)) {
if(!is.null(sentences)) {
if (length(sentences[[p]])!=0){
sent<-c(sent,sentences[[p]])
}
}
}
library(tm)
library(Matrix)
sent.list <- c(sent)
sent.corpus<-Corpus( VectorSource( sent.list ) )
#Convert all letters to lower case.
sent.corpus <- tm_map(sent.corpus, tolower)
#Remove punctuations.
sent.corpus <- tm_map(sent.corpus, removePunctuation)
#Remove list of stop words in Turkish, from corpus.
sent.corpus <- tm_map(sent.corpus, removeWords, stopwords('english'))
#Erase consequential multiple spaces.
sent.corpus <- tm_map(sent.corpus, stripWhitespace)
#Remove stem words.
sent.corpus <- tm_map(sent.corpus, stemDocument)
#Remove numeric number since they won't be meaningfull for our mining process.
sent.corpus <- tm_map(sent.corpus, removeNumbers)
#Convert to plain text document
sent.corpus <- tm_map(sent.corpus, PlainTextDocument)
#sent.dtm <- DocumentTermMatrix( sent.corpus, control = list(weighting = weightTf, normalize = TRUE))
sent.dtm <- DocumentTermMatrix( sent.corpus, control = list(weighting = weightTf, normalize = TRUE) )
rownames(sent.dtm) = 1:nrow(sent.dtm)
sent.tdm <- TermDocumentMatrix( sent.corpus )
colnames(sent.tdm) = 1:ncol(sent.tdm)
if(length(sent)<1){
nCluster<-0
}else{
nCluster<-sqrt(length(sent))#
}
#2)summaritaziton by hierarhic clustering ------------------------
y <- sparseMatrix( i=sent.tdm$i, j=sent.tdm$j, x=sent.tdm$v, dimnames = dimnames(sent.tdm) )
#colnames(y) <- 1:ncol(y)
htree=hclust(dist(t(y)))
plot( htree )
# Compare the 2 and 4 grouping:
g25 <- cutree(htree, k = c(2,5)) #k=2:5
str(sent.hcluster)
#table(grp2 = g25[,"2"], grp5 = g25[,"5"])
#--:
sent.hcluster <- cutree(htree, k = nCluster) #nCluster<-c(2,3)
#---------------------------------------------------------2)
#3)summaritaziton by k means clustering ---------------------------
### k-means (this uses euclidean distance)
sent.dtmmat <- as.matrix(sent.dtm)
### Normalize
#m<-sent.dtmmat
#norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)
#m_norm <- norm_eucl(m)
### cluster into int clusters
sent.kmeans <- kmeans(sent.dtmmat, nCluster)
#table(sent.kmeans$cluster)
#----------------------------------------------------------------3)
#---------------------------------------------------------------------------------find centers of clusters
#str(sent.kmeans$cluster)
#str(sent.hcluster)
#sent[2]
#sent.kmeans$cluster
findCenter <- function(cluster){
clc<-list()
for (i in 1:length(cluster)){
tmp <- try( clc[[cluster[i]]]<-c(clc[[cluster[i]]], i) )
if(class(tmp)=="try-error"){
clc[[cluster[i]]]<-list();
clc[[cluster[i]]]<-c(clc[[cluster[i]]], i)
}
}
return (clc)
}
resc<-findCenter(sent.kmeans$cluster) ################################ sent.kmeans$cluster - sent.hcluster
#resc[[3]] #array of index of doc. in cluster 3
getAvg<-function(ind){ #uses sent.dtmmat
avgdoc<-sent.dtmmat[1,] #term list
centers<-rep.int(0, length(ind) )
for(i in 1:length(ind)){  #for each cluster
for(k in 1:length(avgdoc)){ #initialize average terms list
avgdoc[k]<-0
}
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
for(t in 1:length(avgdoc)){ #for each term
avgdoc[t] <- avgdoc[t] + sent.dtmmat[ as.numeric(ind[[i]][j]) , t]
}
}
#Calculate ranks
rankd<-0
rankd<-rep.int(0, length(ind[[i]]))  #keeps document rank
for(j in 1:length(ind[[i]]) ){ #for each document in cluster
#resc[[i]][j] #each document
tmpRank<-0
for(t in 1:length(avgdoc)){ #for each term
tmpRank<-(sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*sent.dtmmat[ as.numeric(ind[[i]][j]) , t] ) + (avgdoc[t]*avgdoc[t])
#rankd[j] <- rankd[j] + ( sent.dtmmat[ as.numeric(ind[[i]][j]) , t]*avgdoc[t] )
}
if(tmpRank!=0){
rankd[j]<- sqrt( tmpRank )
}else{
rankd[j]<- 0
}
}
#find highest ranked document's index
#centers[i]<- ind[[i]][ which.max(rankd) ]
centers[i]<- ind[[i]][ which.min(rankd) ]
}
return (centers)
}
centers<-getAvg(resc)
summ<-rep.int(0,length(centers))
for(i in 0:length(centers)){
summ[i] <- sent[as.numeric(centers[i])]
}
summ
#---------------------------------------------------------------------------------find centers of clusters
getwd
getwd()
